{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag = 0\n",
    "\n",
    "# if flag == 0:\n",
    "#   !pip install pandas numpy scikit-learn seaborn matplotlib scipy nltk tensorflow keras transformers\n",
    "#   flag = 1\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import concatenate, Concatenate\n",
    "from keras.layers import Input, Embedding, Conv1D, Conv2D, GlobalMaxPooling1D, GlobalAveragePooling1D, Flatten, MaxPooling2D, MaxPooling1D, Dense, Dropout, Reshape\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "import transformers\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers.legacy import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955fe4c",
   "metadata": {},
   "source": [
    "#### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68586b32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('SDSS_DR18.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641704d",
   "metadata": {},
   "source": [
    "#### Check column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611c7896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7eb556",
   "metadata": {},
   "source": [
    "#### Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicate_rows = df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicate rows:\", num_duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414287f6",
   "metadata": {},
   "source": [
    "#### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfb5e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "column_names = df.columns.values.tolist()\n",
    "\n",
    "print(\"Column name \\t Count of missing values \\t Percentage of missing value to total rows\")\n",
    "for col in column_names:\n",
    "    count_nan = df[col].isnull().sum()\n",
    "    pct_nan = count_nan / len(df) * 100\n",
    "    if col in ['ra', 'dec', 'u', 'g', 'r', 'i', 'z']:\n",
    "        print(col + \" - \\t\\t\\t\" + str(count_nan) + \" \\t\\t\\t\\t\" + str(round(pct_nan, 2)) + \"%\")\n",
    "    else:\n",
    "        print(col + \" - \\t\\t\" + str(count_nan) + \" \\t\\t\\t\\t\" + str(round(pct_nan, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad472a",
   "metadata": {},
   "source": [
    "#### Bar representing instances per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1ddec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_counts = df['class'].value_counts()\n",
    "\n",
    "plt.bar(class_counts.index, class_counts.values)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc7bd2",
   "metadata": {},
   "source": [
    "#### Box plots representing the statistical summary of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eebe63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_exclude = ['objid', 'specobjid', 'class']\n",
    "\n",
    "column_names = [col for col in df.columns if col not in columns_to_exclude]\n",
    "\n",
    "num_columns = len(column_names)\n",
    "num_rows = math.ceil(num_columns / 5)\n",
    "\n",
    "fig, axs = plt.subplots(num_rows, 5, figsize=(13, 3 * num_rows))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    current_row = i // 5\n",
    "    position_in_row = i % 5\n",
    "\n",
    "    ax = axs[i]\n",
    "\n",
    "    sns.boxplot(x=df['class'], y=df[column], ax=ax)\n",
    "    ax.set_title(column)\n",
    "    ax.set_ylabel(column)\n",
    "\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    if current_row == 0 and position_in_row < 2:\n",
    "        ax.set_title(column)\n",
    "    else:\n",
    "        current_row += 1\n",
    "\n",
    "for i in range(num_columns, len(axs)):\n",
    "    fig.delaxes(axs[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4b9a2",
   "metadata": {},
   "source": [
    "#### Histograms for representing data distribution across all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07431ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_columns = len(column_names)\n",
    "num_rows = math.ceil(num_columns / 3)\n",
    "\n",
    "plt.figure(figsize=(19, num_rows * 5))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    plt.subplot(num_rows, 3, i + 1)\n",
    "    plt.hist(df[column], bins=30, color='skyblue')\n",
    "    plt.title(column)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abab087",
   "metadata": {},
   "source": [
    "#### Data split for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['class', 'objid', 'specobjid'])\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e078ded3",
   "metadata": {},
   "source": [
    "#### Shuffling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ce670",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed7c31",
   "metadata": {},
   "source": [
    "#### Dataset split into train, test and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e0d3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964bbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e41103",
   "metadata": {},
   "source": [
    "#### Scaling all the numerical data to  standard scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7ecc1",
   "metadata": {},
   "source": [
    "#### Label encoding the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b377341",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "galaxy_index = class_mapping['GALAXY']\n",
    "qso_index = class_mapping['QSO']\n",
    "star_index = class_mapping['STAR']\n",
    "print(\"Galaxy class index:\", galaxy_index)\n",
    "print(\"QSO class index:\", qso_index)\n",
    "print(\"Star class index:\", star_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366bd02",
   "metadata": {},
   "source": [
    "#### One hot encoding the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01881ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_categorical(y_train, num_classes)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15815068",
   "metadata": {},
   "source": [
    "#### Checking number of instances for train, test and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rows = X_train.shape[0]\n",
    "X_test_rows = X_test.shape[0]\n",
    "X_val_rows = X_val.shape[0]\n",
    "y_train_rows = y_train.shape[0]\n",
    "y_test_rows = y_test.shape[0]\n",
    "y_val_rows = y_val.shape[0]\n",
    "\n",
    "print(\"Input for train:\", X_train_rows)\n",
    "print(\"Input for test:\", X_test_rows)\n",
    "print(\"Input for validation:\", X_val_rows)\n",
    "print(\"Target for train:\", y_train_rows)\n",
    "print(\"Target for test:\", y_test_rows)\n",
    "print(\"Target for validation:\", y_val_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d46a0a",
   "metadata": {},
   "source": [
    "#### Checking shape of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24f153",
   "metadata": {},
   "source": [
    "#### Checking shape of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e960f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train shape:\", y_train_one_hot.shape)\n",
    "print(\"y_val shape:\", y_val_one_hot.shape)\n",
    "print(\"y_test shape:\", y_test_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efa96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qso_class_label = label_encoder.transform(['QSO'])[0]\n",
    "qso_data = X_train[y_train == qso_class_label]\n",
    "qso_labels = y_train[y_train == qso_class_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = int(len(qso_data) * 0.5)  # Example ratio, adjust as needed\n",
    "other_data = X_train[y_train != qso_class_label][:subset_size]\n",
    "other_labels = y_train[y_train != qso_class_label][:subset_size]\n",
    "\n",
    "combined_data = np.concatenate([qso_data, other_data])\n",
    "combined_labels = np.concatenate([qso_labels, other_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506282f6",
   "metadata": {},
   "source": [
    "### Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=4232, k_neighbors=3)\n",
    "X_resampled, y_resampled = smote.fit_resample(combined_data, combined_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_qso_resampled = X_resampled[y_resampled == qso_class_label]\n",
    "y_qso_resampled = y_resampled[y_resampled == qso_class_label]\n",
    "\n",
    "X_train_resampled = np.concatenate((X_train[y_train != qso_class_label], X_qso_resampled))\n",
    "y_train_resampled = np.concatenate((y_train[y_train != qso_class_label], y_qso_resampled))\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X_train_resampled))\n",
    "X_train_resampled = X_train_resampled[shuffled_indices]\n",
    "y_train_resampled = y_train_resampled[shuffled_indices]\n",
    "\n",
    "y_train_resampled_one_hot = to_categorical(y_train_resampled, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337d0e1",
   "metadata": {},
   "source": [
    "### Perform Kolmogorov-Smirnov Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "ks_test_results = []\n",
    "\n",
    "for i in range(num_features):\n",
    "    ks_statistic, p_value = ks_2samp(X_train[:, i], X_train_resampled[:, i])\n",
    "    ks_test_results.append({'Feature Index': i,\n",
    "                            'KS Statistic': ks_statistic,\n",
    "                            'P-Value': p_value,\n",
    "                            'Similar Distribution': 'Yes' if p_value >= 0.05 else 'No'})\n",
    "\n",
    "\n",
    "ks_test_results_df = pd.DataFrame(ks_test_results)\n",
    "print(ks_test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5192d2",
   "metadata": {},
   "source": [
    "#### Reshaping data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b831f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled = X_train_resampled.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1b9a1",
   "metadata": {},
   "source": [
    "### Augmented CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaed600",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_resampled.shape)\n",
    "print(y_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c8c55",
   "metadata": {},
   "source": [
    "#### Defining metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d40faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (actual_positives + K.epsilon())\n",
    "\n",
    "def custom_f1_score(y_true, y_pred):\n",
    "    precision_value = precision(y_true, y_pred)\n",
    "    recall_value = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_value * recall_value) / (precision_value + recall_value + K.epsilon()))\n",
    "\n",
    "def fnr(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    false_negatives = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
    "    return false_negatives / (true_positives + false_negatives + K.epsilon())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a8d6e",
   "metadata": {},
   "source": [
    "### CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68bc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(X_train, y_train, X_val, y_val, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(128, 3, activation='relu'))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', precision, recall, custom_f1_score, fnr])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "        \n",
    "    history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = cnn_model(X_train_resampled, y_train_resampled_one_hot, X_val, y_val_one_hot, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7483c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fnr(y_true, y_pred):\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    fnr_per_class = []\n",
    "    for i in range(len(cm)):\n",
    "        FN = cm[i, :].sum() - cm[i, i]\n",
    "        TP = cm[i, i]\n",
    "        fnr = FN / (FN + TP) if (FN + TP) != 0 else 0\n",
    "        fnr_per_class.append(fnr)\n",
    "\n",
    "    avg_fnr = np.mean(fnr_per_class)\n",
    "    return avg_fnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_evaluation(model, X_val, y_val, num_simulations=30):\n",
    "    metric_values = {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'fnr': []}\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        preds = model.predict(X_val)\n",
    "        preds_labels = preds.argmax(axis=1)\n",
    "        y_val_labels = y_val.argmax(axis=1)\n",
    "\n",
    "        metric_values['accuracy'].append(accuracy_score(y_val_labels, preds_labels))\n",
    "        metric_values['precision'].append(precision_score(y_val_labels, preds_labels, average='macro'))\n",
    "        metric_values['recall'].append(recall_score(y_val_labels, preds_labels, average='macro'))\n",
    "        metric_values['f1_score'].append(f1_score(y_val_labels, preds_labels, average='macro'))\n",
    "        metric_values['fnr'].append(calculate_fnr(y_val_labels, preds_labels))\n",
    "\n",
    "    metric_stats = {metric: {'mean': np.mean(metric_values[metric]), 'std': np.std(metric_values[metric])} for metric in metric_values}\n",
    "\n",
    "    return metric_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc63c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training the model\n",
    "metric_stats = monte_carlo_evaluation(model, X_val, y_val_one_hot, num_simulations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96779cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metric_stats:\n",
    "    print(f\"{metric}: Mean = {metric_stats[metric]['mean']}, Std Dev = {metric_stats[metric]['std']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', 'precision', 'recall', 'custom_f1_score', 'fnr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25853137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    print(metric, \":\", history.history[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fe770",
   "metadata": {},
   "source": [
    "### Visualizing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2bf31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for metric in metrics:\n",
    "    plt.plot(history.history[metric], label=metric)\n",
    "\n",
    "plt.title('Model Performance Metrics Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b576e06",
   "metadata": {},
   "source": [
    "#### Model evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11872b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_evaluation = model.evaluate(X_test, y_test_one_hot)\n",
    "print(\"Test Set Evaluation Metrics:\", test_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loss, test_accuracy, test_precision, test_recall, test_f1_score, test_fnr = model.evaluate(X_test, y_test_one_hot)\n",
    "y_pred = model.predict(X_test).argmax(axis=1)\n",
    "y_true = y_test_one_hot.argmax(axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1 Score: {test_f1_score}\")\n",
    "print(f\"Test FNR: {test_fnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c24d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_metrics_and_confusion_matrix(test_metrics, confusion_matrix, class_labels):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    metrics = list(test_metrics.keys())\n",
    "    values = [test_metrics[metric] for metric in metrics]\n",
    "    plt.bar(metrics, values, color='skyblue')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Test Metrics')\n",
    "    for i, value in enumerate(values):\n",
    "        plt.text(i, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_metrics = {\n",
    "    'accuracy': test_accuracy,\n",
    "    'precision': test_precision,\n",
    "    'recall': test_recall,\n",
    "    'f1_score': test_f1_score,\n",
    "    'fnr': test_fnr\n",
    "}\n",
    "\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "plot_test_metrics_and_confusion_matrix(test_metrics, cm, class_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytfenv",
   "language": "python",
   "name": "mytfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
